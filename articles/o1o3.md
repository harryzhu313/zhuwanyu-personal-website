---
title: 从 o1 到 o3：一场 AI 思维的范式跃迁
date: 2025-12-25
categories: [深度学习]
cover: https://image.harryrou.wiki/2025-12-25-n3jdc9n3jdc9n3jd.jpg
description: ""
---


![](https://image.harryrou.wiki/2025-12-25-1.jpeg)

在过去几年里，我们被 GPT-4 这样的大语言模型惊艳，是因为它们说话太漂亮了。它们就像一个博览群书的文科生，能够引经据典，流畅地回答你的问题。但如果你仔细观察，你会发现它们有时在“胡说八道”。因为本质上，传统的**大语言模型（LLM）** 只是在做一件事：**预测下一个 token（字/词）**。它是基于概率的统计学奇迹，是对人类过往文本的模仿。

![](https://image.harryrou.wiki/2025-12-25-2.jpeg)

然而，OpenAI 的 o1 和 o3 模型的出现，标志着一种新的范式。它们不再仅仅凭直觉给出回答，而是会在回答之前进行**推理**。这正是推理模型（Reasoning Models）出现的背景。

## 什么是“推理模型”？

### 慢思考：理性脑的回归

![](https://image.harryrou.wiki/2025-12-25-3.jpeg)

如果把之前的 LLM 比作人类的“直觉”（System 1，快思考），那么推理模型就是在试图构建人类的“理性”（System 2，慢思考）。

当你被问到“2+2等于几”时，你脱口而出“4”，这是 System 1，是大语言模型擅长的。但当你被问到“17乘以24等于几”时，你必须停下来。你不能只靠直觉，你需要在脑海中建立一个算式，一步步推导。这个过程，就是**逻辑思考**。

o1的突破，正是让AI从"系统1"跨入了"系统2"。它不再只是预测下一个token，而是预测下一个**思维步骤**。在给出答案之前，它会在内部展开一条"思维链"（Chain of Thought），尝试多种路径，评估各种可能，最终收敛到最优解。

![](https://image.harryrou.wiki/2025-12-25-4.jpeg)

### 推理模型 vs. 大语言模型

![](https://image.harryrou.wiki/2025-12-25-5.jpeg)

传统的大语言模型（LLM）受训的目标很简单——根据已有上下文预测下一个词。它们生成内容更像是在瞬间完成，无暇顾及详细的逻辑步骤。而推理模型则截然不同：它们训练有素，会先生成一连串**中间推理过程**，再得出最终答案。换言之，推理模型试图**预测下一段“思维链CoT”**，而不仅仅是下一个词。

> “思维链”（Chain-of-Thought, CoT）就是模型解决问题时内部产生的一系列推理步骤或想法。

举个例子，在回答一个复杂数学题时，传统模型可能胡乱猜一个数字；而推理模型会先列出详细的解题步骤，逐步演算，直到推导出正确答案。

### 算力的“双轮”驱动

![](https://image.harryrou.wiki/2025-12-25-6.jpeg)

推理模型的优秀表现来自两个方面：
- **训练阶段**投入大量算力进行强化学习调优[[强化微调]]
- **推理阶段**允许模型投入更多计算量来“思考”[[test-time compute]]。

OpenAI 的研究表明，给 o1 模型喂入更多强化学习训练（即增加训练时间和轮次），以及在推理时允许它想得更久，性能都会持续提升。这与传统LLM形成对比——过去提升模型主要靠加大模型规模和训练数据，而推理模型则另辟蹊径，通过**让模型在推理时做更多计算**来提高解题能力。你可以把这种做法想象成在考试中，允许考生用更长的草稿演算过程或多次尝试，这样自然更容易找到正确答案。

## 强化学习：让AI学会“试错”推理

![](https://image.harryrou.wiki/2025-01-21-104756.jpg)


推理模型背后的关键技术之一是**强化学习（Reinforcement Learning）**。不同于一次性学习固定模式，强化学习让模型通过**与环境反复交互**、不断试错来逐步改进策略。这一思想可以追溯到一个多世纪前心理学家的动物实验：美国心理学家爱德华·桑代克(Edward Thorndike)曾把饥饿的猫关在“迷箱”里，猫需要摸索按下某个杆才能逃出进食。实验发现，**猫完全是靠反复试验和错误**才学会如何逃脱——每次碰巧按对杆就得到食物奖励，错误尝试则渐渐被淘汰。后来行为学家B.F.斯金纳(B.F. Skinner)发明了著名的**斯金纳箱([[斯金纳盒子 skinner box]])**，让动物通过按杆获取食物或避免噪音惩罚，进一步验证了**奖赏会强化行为**、惩罚会抑制行为的原理。这些实验揭示出的“**试错学习**”机制，正是强化学习算法的灵感来源。

![](https://image.harryrou.wiki/2025-12-25-Skinnerbox.jpeg)

在强化学习框架中，我们把AI模型看作一个**智能体（agent）**，它不断地**与环境交互**：每当智能体采取一个动作，环境都会给予一个**反馈**（奖励或惩罚）。智能体会根据反馈调整自己的策略，目的是长期累计的奖励最大化。简单来说，**“对”的行为得到奖赏，下次就更可能重复；“错”的行为得不到奖赏甚至受惩罚，下次出现概率就降低。”** 通过无数轮这样的交互试验，智能体逐渐学会完成任务的最优策略。

### 用强化学习训练推理链（RLFT）

![](https://image.harryrou.wiki/2025-12-25-7.jpeg)

那么，推理模型是如何借助强化学习来“学会思考”的呢？OpenAI o1 模型的训练过程可以做一个典型说明。首先，研究者让模型针对大量问题进行**自由的推理尝试**：就像人类学生考前狂刷题一样，模型会对每个问题生成上百上千条不同的**思维链**（CoT）作为候选解题思路。这些思维链包括逐步推导、分解问题、尝试各种解法，甚至自我检查和修正错误等过程。然后，引入强化学习的反馈机制：对于模型给出的**最终答案**，如果能够通过程序自动验证其正确性，就给予模型一个**正向奖励**；反之则不给奖励甚至给予负反馈。例如，在数学题上可以直接检查模型答案是否与标准答案字符串匹配，或在编程题中运行模型生成的代码看是否通过所有测试用例。模型由此“知道”了哪些思维链导出了正确结果，哪些走向了歧途。

通过无数轮这样的自我博弈，模型的参数会朝着**更倾向产出正确推理过程**的方向更新——这就是**强化学习微调（RL Fine-Tuning, RLFT）** 的精髓。

> 简单说，模型**试遍各种解题思路**，胜出的正确思路得到强化保留，错误思路被弱化舍弃。

久而久之，模型在训练集中学会了**沿着正确的逻辑链条思考**，大大提高了复杂推理任务的准确率。OpenAI 的技术报告显示，经过这样强化训练的 o1 模型在一系列数学和科学竞赛题上表现远超 GPT-4 等传统模型：例如在美国数学奥赛预选赛(AIME)中，GPT-4o仅答对13%的题，而o1正确率高达83%。这充分证明了RLFT策略在推理领域的威力。


### RLFT vs. RLHF：自我优化与人类指引

![](https://image.harryrou.wiki/2025-12-25-9.jpeg)


RLFT 和 RLHF 都属于强化学习范畴，但在推理模型训练中各有用武之地：

- **RLFT（自我验证反馈）**：模型依赖**可程序判定**的正确信号作为奖励。优点是客观准确、高效，模型能学到精确求解问题的能力。例如 o1 通过 RLFT 学会了卓越的数学和编程推理。但局限是只能用于有明确验证标准的任务；对于需要主观判断的任务，机器给不出可靠的奖惩信号。
    
- **RLHF（人类偏好反馈）**：模型以**人类的直观评判**为学习信号。它可以让AI符合人类价值观，在开放领域产生令人满意的回答，这是ChatGPT 系列成功的关键。但RLHF也有风险：人类反馈毕竟带有主观倾向，且收集成本高，还可能出现“奖励异常”（模型投机取巧迎合人类而非真正理解）。对推理模型来说，研究者往往**谨慎使用RLHF**。事实上，许多最新推理模型**刻意避免**使用纯粹的神经网络奖励模型（即RLHF中的那套人评奖赏），以防止模型学到一些投机取巧的“旁门左道”。相反，它们更青睐RLFT这类**基于可验证客观标准**的强化信号，在此基础上再辅以少量人类反馈做最后的打磨和对齐。这样既保证了逻辑推理的硬实力，又兼顾一定的友好度。
    

总结来说，RLFT让模型像**自学成才**一样反复试错提升逻辑，而RLHF更像**导师点拨**让模型合乎人意。两者结合，赋予了推理模型既**擅长解题**又**贴近人类期望**的双重能力。

## 从o1到o3：AI从思考者进化为“智能体”


OpenAI 的 o1 和 o3 模型展现了推理模型的两次飞跃。o1证明了AI可以通过延长思考链条获得惊人的解题能力，而o3则更进一步，让AI具备了类似 **智能体（agent）** 的特质，能够自主规划并调用工具来完成复杂任务。

得益于**长链条推理+强化学习调教**，o1 在许多**可验证的任务**上达到了前所未有的高度。比如在数学竞赛、编程竞赛以及博士水平的科学问答中，o1 大幅**碾压**了当时最强的GPT-4模型。然而，o1 也有明显的**短板**：为了追求推理能力，它在训练中主要聚焦数学、科学、编程这些有客观标准的领域，对日常知识问答、开放对话等**广谱知识**和**工具使用**方面不够擅长。正如OpenAI发布博客所言，o1 作为早期模型还缺乏ChatGPT那样的实用功能，比如联网搜索信息、处理文件图像等。因此OpenAI 在短时间内推出了 o3 模型。

### OpenAI o3：向自主智能体迈进

![](https://image.harryrou.wiki/2025-12-25-11.jpeg)


o3 是OpenAI在推理模型道路上的再一次飞跃。与o1注重于**闭门苦思**不同，o3开始赋予模型**面向外部世界行动**的能力，使其逐渐变成一个可以自主规划、执行任务的**智能体**。在o3的发布中，OpenAI首次宣布模型具备**Agentic tool use**（代理式工具使用）能力。具体来说，o3 可以**自主决定何时以及如何调用各种工具**来辅助完成任务。这些工具包括：**网络搜索**（当需要查找新信息时），**Python代码执行**（当需要计算或数据分析时），**图像分析**（当输入含有视觉信息时，o3会将图像纳入推理链），甚至**生成图像**来辅助思考。换句话说，o3 不再局限于模型内部的文字推理，它可以**把外部世界当作自己思考过程的延伸**。

> 智能体：针对一个目标，能够进行推理->计划->执行（使用工具）->反馈->调整计划->执行的过程可以称为 AI agent。

![](https://image.harryrou.wiki/2025-12-25-12.jpeg)

具备工具使用后，o3 变得更像一个真正的智能体：面对一个复杂任务，它会**先规划**解题思路，必要时**查询资料**或**调用函数**获取额外信息，随后**综合信息作出推理**，再**执行下一步计划**，如此循环直到完成任务。这种 **“计划-行动-反馈”** 的循环让 o3 能处理**多步骤、跨领域**的挑战，如同一个自主的AI助手可以替你上网搜索、算账、看图然后给出结论。例如，在回答一个商业分析问题时，o3 可能会先列出分析步骤，然后自动搜索相关市场数据，用Python跑一些统计，再总结出洞见，最后给出结构化的报告回答。

![](https://image.harryrou.wiki/2025-12-25-13.jpeg)

o3 标志着推理模型从“纸上谈兵”进化到了“上手实干”。它兼具**深度思考**和**动手能力**：既能利用自身的CoT链条进行严谨推理，又能通过工具获取新知、执行操作。这正是迈向**通用人工智能代理**的重要一步——模型不只是被动回答问题，而是可以主动为你完成复杂任务。

## 尾声：推理模型的意义

![](https://image.harryrou.wiki/2025-12-25-2024-12-30-073941.png)

推理模型的出现，让AI研究者看到了一条更清晰的路径：通过强化学习赋予模型"思考"能力，通过思维链让推理过程可解释，通过Agent架构让AI能够与真实世界交互。

这不是终点，但可能是最重要的路标之一。

